# Part I 基础

## 问题陈述

### 2.1 元学习问题及其术语

​		小样本元学习的目标是训练这样一个模型，该模型只需使用少量数据和训练迭代次数便可快速应用于新任务。为了实现这个目标，模型或学习器在元学习阶段便针对一组任务进行训练，这使得训练有素的模型可以仅使用少量示例或试验便可以快速适应新任务。实际上，元学习问题将整个学习任务视为训练任务（<!--传统学习，所学习的是模型中的参数，而元学习是学习如何去学习。是这个意思么）-->。在本节中，我们将以一般方式将元学习问题设定正形式化，这其中包含不同学习领域的简要示例。

​		我们可以考虑这样一个模型，将其记为$f$,输入的观测值为$x$,输出值为$y$, 便有$y = f(x)$。在元学习中，该模型经过训练，能够适应大量或无限数量的任务。我们希望我们的框架能够应用于各种学习问题，从分类任务到强化学习，下面我们将介绍一个学习任务的一般概念。正式的，对于每一个任务，我们将其定义为$ \tau $,他的具体定义的内容为以下形式：$ \tau  = \{L(\theta,D),\rho(x_1), \rho(x_{t+1}\mid x_t,y_t), H \}$。其中$L$表示损失函数，其输入为模型的参数$\theta$以及一个训练的数据集$D$，一个初始的观察的分布$\rho(x_1)$,一个迁移过来的分布$\rho(x_{t+1}\mid x_t,y_t)$,以及迭代长度$H$.在独立同分布的有监督学习问题中，当$H = 1$时且输入的数据集$D$包含样本的标签，则对于输出的数据对可以表示为$D = \{(x_1, y_1)^{(k)}\}$。然而在强化学习任务中，对于模型$f$,在第$H$次产生的样本的过程中，对于每一个时刻$t$将选择一个输出，记为$\hat{y}_t$；因此，传递给损失函数的数据集由模型推出的轨迹组成：$D = \{(x_1,\hat{y}_1, \cdots,x_H,\hat{y}_H)^{(k)}\}$。损失函数记为$L(\theta, D) \rightarrow \mathbb{R}$，为模型$f_\theta$针对特定任务提供反馈，这个损失函数也许可以是马尔可夫决策过程中错误分类损失或成本函数的形式。

​		在我们的元学习设计中，考虑这样一个想要用我们的模型去适应的*分配的任务*$p(\tau)$。<font color="#dd0000">*<!--（a distribution task,我不知道我翻译的对不对。）-->*</font>在k样本学习（小样本学习叫做$few-short \quad learning$ ）的设定中，模型被训练并用于学习一个新的任务$\jmath_i$,基于一个分配的任务$p(\tau )$，并仅仅使用源自于$q_i$*<!--（这个q是啥？）-->*的K个样本,记为$D_i^{tr}$,并反馈由$\tau_i$产生的损失值$L_{\tau_i}$。在元训练中，一个任务$\tau_i$取样于一个分配的任务$p(\tau)$,模型使用$k$个样本进行训练，，并使用来自于$\tau_i$所对应的损失函数$L_{\tau_i}$,随后利用从$\tau_i$抽取的新样本对模型进行测试，我们将这样的数据记为$D_i^{test}$。然后通过观察新数据$D_i^{test}$在模型$f$上的测试误差来调整参数，从而改进模型$f$。实际上，在采样任务$\tau_i$上的测试误差将作为元学习过程中的训练误差。在元学习的最后，新的任务将从$p(\tau)$中采样。元学习的表现将在从K个样本中学习之后，根据训练出的元模型的表现进行衡量。通常，元学习训练和测试的过程中的任务目标将保持不变。

​		在表1中，我们概述了本文档中使用的元学习术语和符号。 本质上，元学习算法学习使用元训练集中的任务的数据来学习任务。元学习完成后再通过元学习的训练集来测试模型的效果。我们将广义地使用“任务”这个术语来封装要学习的概念，要适应的域或其组合的东西。

|          符号          |         术语         |                         例子或者细节                         |
| :--------------------: | :------------------: | :----------------------------------------------------------: |
|         $\tau$         |         任务         | 对应于目标，领域，环境或其组合需要被学习或适应的实体<br />就是对应的学习的模型要处理的任务。 |
|       $P(\tau)$        | 任务（特征的）的分布 |          从元训练和元测试任务的找出任务特征的分布。          |
| {$\tau_i$}~$P\{\tau\}$ |      元学习任务      |                  用于进行元学习的一组任务。                  |
|     {$D_{\tau_i}$}     |       元训练集       | 元学习任务相对应的一组数据集，算法将针对这组数据学习如何进行学习。 |
| {$\tau_j$}~$P\{\tau\}$ |      元测试任务      | 用于评估的一组任务，学习出的模型将在这些数据上评估其学习的能力。 |
|     {$D_{\tau_j}$}     |       元测试集       |                元测试任务相对应的一组数据集。                |
|     $D_\tau^{tr}$      |   训练集（支撑集）   | 针对任务$\tau$的训练集，通常是从$D_{\tau}$取样$k$个数据点。  |
|      $D_j^{test}$      |   测试集（查询集）   |          针对任务$\tau$的测试数据，取样于$D_{\tau}$          |

<center>表1：本论文中有关于元学习的术语,其他文献中有时使用的术语显示在括号中<\center>

### 2.2 元学习任务的学习空间

​		正如我们前面所讨论，在有监督的元学习的设定中，对于每一个任务$\tau$,其测试集$D_{\tau}^{train}$以及训练集$D_{\tau}^{test}$都是有标记的。然而在元强化学习中，每一个任务的数据集由强化学习的策略所计算出的结果组成。在章节8.1.2部分，我们将探索一组训练及测试数据及其对应的任务目标，以及数据如何将其应于用于逆强化学习的演示。对于每一个例子，训练以及测试数据以及基于这些数据的学习任务都是一致的，但是请注意，以上的这一假设，并不一定是真的，即并不一定是前提。适当的元学习的唯一限制是以下两点：

- （a）用于模型训练的训练集相对于求解测试集是有用的。
- （b）测试集及其相应的学习目标允许优化所需的学习结果。

​        我们通过对训练和测试数据使用不同的策略梯度估计方式，在4.2.2节中以简单的方式利用这一观察结果。但是，这个观察结果非常有趣，因为它为可能的元学习实例和算法开辟了广阔的设计空间：通过监督学习的方式来学习如何学习。甚至不需要以标准方式监督训练数据。正如我们将在7.2及8.2中所展示的，我们可以通过强有力的监督来学习弱监督。在这个情况下，每一个任务的训练数据都是弱监督的，与此相反的是对于测试数据而言却包含有监督的。凭借这种方式，我们能够构造一种仿照元学习算法环境的方式，这些环境包括：如何使用全监督机器人演示来学习人类视频（参见章节7.2）；学习只从积极的成功例子中学习奖励功能（参见章节8.2）；一些人还可以更进一步，使用无监督的训练数据以及有监督的测试数据去学习使用有监督数据的无监督学习的算法。正如Metz et al. (2018）所建议的那样。